{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = os.getenv('db_user')\n",
    "db_password = os.getenv('db_password')\n",
    "db_host = os.getenv('db_host')\n",
    "db_name = os.getenv('db_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to download external data from wiki and store in a vector db and retrieve relevant info "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include check within method to check for existing content in vectorDb if so dirctly jump to querying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_embedding(text):\n",
    "\n",
    "    client = openai.OpenAI(api_key = os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding  # Correct way to access embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import streamlit as st\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "\n",
    "def extractData_loadData_performSemanticSearch(info, user_query, history=[]):\n",
    "\n",
    "    query = f\"Select count(*) from metadata where dish_name ILIKE '{info}'\"\n",
    "    insert_query = f\"Insert into metadata Values('{info}')\"\n",
    "    db = SQLDatabase.from_uri(f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\")\n",
    "    execute_query = QuerySQLDataBaseTool(db=db)\n",
    "    count = execute_query.invoke(query)\n",
    "\n",
    "    if count == '[(0,)]':\n",
    "        # Initialize Wikipedia API\n",
    "        wiki_wiki = wikipediaapi.Wikipedia(user_agent='Menudata.ai', language='en')\n",
    "\n",
    "        # Fetch the page\n",
    "        page_py = wiki_wiki.page(info)\n",
    "\n",
    "        # Check if the page exists\n",
    "        if not page_py.exists():\n",
    "            return False\n",
    "\n",
    "        # Function to create chunks from sections and summary\n",
    "        def create_chunks_from_page(page, dish_name):\n",
    "            chunks = []\n",
    "\n",
    "            # Add the page summary as a chunk\n",
    "            summary_chunk = {\n",
    "                \"text\": f\"{page.summary[:200]}\\n\\nSource: {page.canonicalurl}\",  # Append URL\n",
    "                \"metadata\": {\n",
    "                    \"dish_name\": dish_name,\n",
    "                    \"section\": \"Summary\",\n",
    "                }\n",
    "            }\n",
    "            chunks.append(summary_chunk)\n",
    "\n",
    "            # Recursively add sections and subsections as chunks\n",
    "            def add_sections_to_chunks(sections, parent_section=None, level=0):\n",
    "                for s in sections:\n",
    "                    # Create a chunk for the section\n",
    "                    section_title = f\"{parent_section} - {s.title}\" if parent_section else s.title\n",
    "                    section_chunk = {\n",
    "                        \"text\": f\"{s.text[:200]} Source: {page.canonicalurl}\",  # Append URL\n",
    "                        \"metadata\": {\n",
    "                            \"dish_name\": dish_name,\n",
    "                            \"section\": section_title,\n",
    "                        }\n",
    "                    }\n",
    "                    chunks.append(section_chunk)\n",
    "\n",
    "                    # Add subsections recursively, passing the current section as the parent\n",
    "                    add_sections_to_chunks(s.sections, parent_section=s.title, level=level + 1)\n",
    "\n",
    "            # Start processing sections\n",
    "            add_sections_to_chunks(page.sections)\n",
    "\n",
    "            return chunks\n",
    "\n",
    "        # Create chunks from the page\n",
    "        chunks = create_chunks_from_page(page_py, info)\n",
    "        print(\"Chunks created!\")\n",
    "\n",
    "        # Generate embeddings for metadata only\n",
    "        chunk_embeddings = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Convert metadata to a string (e.g., JSON or concatenated string)\n",
    "            metadata_str = f\"{chunk['metadata']['dish_name']} {chunk['metadata']['section']}\"\n",
    "            \n",
    "            # Generate embedding for the metadata string\n",
    "            metadata_embedding = get_embedding(metadata_str)\n",
    "            \n",
    "            chunk_embeddings.append({\n",
    "                \"id\": f\"chunk_{info}{i+1}\",  # Unique ID for each chunk\n",
    "                \"vector\": metadata_embedding,  # Embedding of the metadata\n",
    "                \"metadata\": chunk[\"metadata\"],  # Original metadata\n",
    "                \"text\": chunk[\"text\"]  # Optional: Store text if needed for retrieval\n",
    "            })\n",
    "        print(\"Embeddings created!\")\n",
    "    \n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.Client()\n",
    "\n",
    "    # # Directory where the Chroma DB will be persisted\n",
    "    # persist_directory = \"./chroma_db\"  # Replace with your desired directory\n",
    "\n",
    "    # # Initialize the Chroma client with persistence \n",
    "    # chroma_client = chromadb.Client(\n",
    "    #     Settings(\n",
    "    #     persist_directory=persist_directory,  # Enable persistence\n",
    "    #     chroma_db_impl=\"duckdb+parquet\",      # Use DuckDB with Parquet for storage\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # Create a collection (similar to a table in a traditional DB)\n",
    "    collection_name = 'ExternalInformationTest2'\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "    except:\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "    if count == '[(0,)]':\n",
    "        # Add embeddings to the collection\n",
    "        for chunk in chunk_embeddings:\n",
    "            collection.add(\n",
    "                ids=[chunk[\"id\"]],  # Unique ID for each chunk\n",
    "                embeddings=[chunk[\"vector\"]],  # Embedding of the metadata\n",
    "                metadatas=[chunk[\"metadata\"]],  # Original metadata\n",
    "                documents=[chunk[\"text\"]]  # Optional: Store text if needed for retrieval\n",
    "            )\n",
    "\n",
    "        print(\"Metadata embeddings stored in Chroma!\")\n",
    "        execute_query.invoke(insert_query)\n",
    "\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(' '.join(history)+' '+user_query)\n",
    "\n",
    "    # Perform similarity search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],  # Embedding of the user query\n",
    "        n_results=5 # Number of results to return\n",
    "    )\n",
    "\n",
    "    relevant_documents = results['documents']\n",
    "\n",
    "    return relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created!\n",
      "Embeddings created!\n",
      "Metadata embeddings stored in Chroma!\n"
     ]
    }
   ],
   "source": [
    "result = extractData_loadData_performSemanticSearch('sushi', 'where did sushi originate?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'The earliest written mention of sushi in English described in the Oxford English Dictionary is in an 1893 book, A Japanese Interior, where it mentions sushi as \"a roll of cold rice with fish, sea-weed Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'Until the early 19th century, sushi slowly changed with Japanese cuisine. The Japanese started eating three meals a day, rice was boiled instead of steamed, and of large importance was the development Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'During the Edo period (1603–1867), a third type of sushi, haya-zushi (早寿司、早ずし, \"fast sushi\"), was developed. Haya-zushi differed from earlier sushi in that instead of lactic fermentation of rice, vine Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  ' Source: https://en.wikipedia.org/wiki/Sushi']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to classify user query as Tier 1, 2 or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "\n",
    "def classify_query(user_query, history=[]):\n",
    "    # Step 1: Define the prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''I have a scenario where I want to classify query into a category.\n",
    "\n",
    "        Tier 1: Questions related to restaurants, finding food, or food trends.\n",
    "        Examples:\n",
    "        Which restaurants in San Francisco offer dishes with Impossible Meat? (Label - Tier 1)\n",
    "        Give me a summary of the latest trends around desserts. (Tier 1)\n",
    "        Which restaurants are known for sushi? (Tier 1)\n",
    "        Compare the average menu price of vegan restaurants in LA vs. Mexican restaurants. (Tier 1)\n",
    "        Which food can I find with peas?\n",
    "\n",
    "        Tier 2: Questions about a dish or ingredients.\n",
    "        Examples:\n",
    "        Tell me about biryani. (Tier 2)\n",
    "        What is the history of sushi? (Tier 2)\n",
    "        Tell me the contents of sushi. (Tier 2)\n",
    "        \n",
    "        Tier 3: Questions that combine both restaurant-related and dish-related queries.\n",
    "        Example:\n",
    "        What is the history of sushi, and which restaurants in my area are known for it?\n",
    "\n",
    "        Tell me which class the query falls into with just one word that is the class. Note – might include chat history which should not be considered for classification but only for context\n",
    "\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "    # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/9w2s326d5cx3n3gsd9wtv04h0000gn/T/ipykernel_22453/1204639571.py:37: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
      "/var/folders/7_/9w2s326d5cx3n3gsd9wtv04h0000gn/T/ipykernel_22453/1204639571.py:40: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "/var/folders/7_/9w2s326d5cx3n3gsd9wtv04h0000gn/T/ipykernel_22453/1204639571.py:43: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain.run({\"history\": history, \"user_query\": user_query})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tier 3'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_query(\"What is italian cusine and where can i find it near me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1 - Generate sql query --> Query DB --> Return result in naturla language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Which restaurants in San Francisco offer dishes with Impossible Meat?\",\n",
    "        \"query\": \"SELECT restaurant_name, city FROM restaurants WHERE city = 'San Francisco' AND id IN (SELECT restaurant_id FROM menu_items WHERE menu_description ILIKE '%impossible%');\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find restaurants near me that serve gluten-free pizza.\",\n",
    "        \"query\": \"SELECT restaurant_name FROM restaurants WHERE id IN (SELECT restaurant_id FROM menu_items WHERE menu_description ILIKE '%gluten free%' AND menu_item ILIKE '%pizza%');\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Give me a summary of the latest trends around desserts.\",\n",
    "        \"query\": \"SELECT mi.menu_item AS dessert_name, COUNT(r.id) AS restaurant_count, AVG(r.rating) AS avg_rating, SUM(r.review_count) AS total_reviews FROM menu_items mi JOIN restaurants r ON mi.restaurant_id = r.id WHERE mi.menu_category ILIKE '%dessert%' OR mi.categories ILIKE '%dessert%' GROUP BY mi.menu_item ORDER BY total_reviews DESC, avg_rating DESC LIMIT 10;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Which restaurants are known for Sushi?\",\n",
    "        \"query\": \"SELECT r.restaurant_name, r.rating FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE m.menu_item ILIKE '%sushi%' AND r.rating >= 4.0 GROUP BY r.restaurant_name, r.rating ORDER BY r.rating DESC;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Compare the average menu price of vegan restaurants in LA vs. Mexican restaurants.\",\n",
    "        \"query\": \"SELECT category, AVG(CASE WHEN price = '$' THEN 1 WHEN price = '$$' THEN 2 WHEN price = '$$$' THEN 3 WHEN price = '$$$$' THEN 4 END) AS avg_price_level FROM (SELECT r.id AS restaurant_id, CASE WHEN m.categories ILIKE '%vegan%' THEN 'Vegan' WHEN m.categories ILIKE '%mexican%' THEN 'Mexican' END AS category, r.price FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE r.city = 'Los Angeles' AND (m.categories ILIKE '%vegan%' OR m.categories ILIKE '%mexican%')) subquery GROUP BY category;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Top 5 famous desserts in Boston based on reviews.\",\n",
    "        \"query\": \"SELECT m.menu_item, SUM(r.review_count) AS total_reviews FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE r.city = 'Boston' AND (m.menu_category ILIKE '%dessert%' OR m.categories ILIKE '%dessert%') GROUP BY m.menu_item ORDER BY total_reviews DESC LIMIT 5;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Average rating of Mexican and Italian restaurants.\",\n",
    "        \"query\": \"SELECT CASE WHEN m.categories ILIKE '%mexican%' THEN 'Mexican' WHEN m.categories ILIKE '%italian%' THEN 'Italian' END AS cuisine, AVG(r.rating) AS avg_rating FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id AND (m.categories ILIKE '%mexican%' OR m.categories ILIKE '%italian%') GROUP BY cuisine;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Restaurants with the most vegetarian options.\",\n",
    "        \"query\": \"SELECT r.restaurant_name, r.city, COUNT(m.item_id) AS vegetarian_item_count FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE m.categories ILIKE '%vegetarian%' GROUP BY r.restaurant_name, r.city ORDER BY vegetarian_item_count DESC LIMIT 5;\"\n",
    "    }\n",
    "]\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def get_example_selector():\n",
    "    example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "        examples,\n",
    "        OpenAIEmbeddings(),\n",
    "        Chroma,\n",
    "        k=3,\n",
    "        input_keys=[\"input\"],\n",
    "    )\n",
    "    return example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate,PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def tierOne(user_query, history=[]):\n",
    "    \n",
    "    select_table = ['menu_items', 'restaurants']\n",
    "    db = SQLDatabase.from_uri(f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\", include_tables = select_table)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\\nSQLQuery:\"),\n",
    "        (\"ai\", \"{query}\"),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=get_example_selector(),\n",
    "    input_variables=[\"input\", 'top_k'],\n",
    "    )\n",
    "\n",
    "    final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query to run. Unless otherwise specificed.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\"),\n",
    "        few_shot_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "    Chat History:{history}\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer: \"\"\"\n",
    "    )\n",
    "\n",
    "    generate_query = create_sql_query_chain(llm, db,final_prompt) \n",
    "    execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "    rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "    chain = (\n",
    "    RunnablePassthrough.assign(query=generate_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | rephrase_answer )\n",
    "\n",
    "    return chain.invoke({'question':user_query, 'history':history})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 15:06:21.522 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-04 15:06:21.559 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-04 15:06:21.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-04 15:06:21.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-04 15:06:22.069 Thread 'Thread-56': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-04 15:06:22.070 Thread 'Thread-56': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-04 15:06:22.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-04 15:06:22.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "answer = tierOne('Where can i find veg food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can find vegetarian or vegan food at the following restaurants in San Francisco: Vegan Mob, Udupi Palace, Pinche Sushi, Beloved Cafe, Healthyish Republic, Mission Curry House, Cha-Ya San Francisco, and Indochine Vegan.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract ingredinet or dish from user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "\n",
    "def extract_info(user_query, history=[]):\n",
    "    # Step 1: Define the prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''You know that the query below is asking information about some dish or ingredient. Identify the ingredient or dish name and return only that value.\n",
    "        Examples -\n",
    "        Tell me about biryani. Biryani \n",
    "        What is the history of sushi? Sushi\n",
    "        Tell me about Saffron. Saffron\n",
    "        Below is the query for you to identify and includes chat history if any,\n",
    "        Note - Add a underscore if multiword \n",
    "        \n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "    # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pasta'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('What is the most common ingredients used in pasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use extractData_loadData_performSemanticSearch method to retrieve relevant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the most common ingredients used in sushi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = extractData_loadData_performSemanticSearch(extract_info(query), query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['All sushi has a base of specially prepared rice, complemented with other ingredients. Traditional Japanese sushi consists of rice flavored with vinegar sauce and various raw or cooked ingredients. Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'Sushi-meshi (鮨飯) (also known as su-meshi (酢飯), shari (舎利), or gohan (ご飯)) is a preparation of white, short-grained, Japanese rice mixed with a dressing consisting of rice vinegar, sugar, salt, and occ Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'Sushi is commonly eaten with condiments. Sushi may be dipped in shōyu (soy sauce), and is usually flavored with wasabi, a piquant paste made from the grated stem of the Wasabia japonica plant. Japanes Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'The dark green seaweed wrappers used in makimono are called nori (海苔). Nori is a type of red algae, typically in the family Bangiaceae, traditionally cultivated in the harbors of Japan. Originally, al Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'The ingredients used inside sushi are called gu and are, typically, varieties of fish. For culinary, sanitary, and aesthetic reasons, the minimum quality and freshness of fish to be eaten raw must be  Source: https://en.wikipedia.org/wiki/Sushi']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer query using context found above and return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_nl(context, user_query, history = []):\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"history\", \"user_query\"],\n",
    "        template='''Using relevant context passed answer the user query. If history of chat availableuse it for relevant context. Note - After answering the query provide relevant source\n",
    "        Relevant Context - {context}\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "     # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"context\": context, \"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common ingredients used in sushi are specially prepared rice known as Sushi-meshi, which is a preparation of white, short-grained, Japanese rice mixed with a dressing consisting of rice vinegar, sugar, salt. The ingredients used inside sushi are called gu and are typically varieties of fish. The minimum quality and freshness of fish to be eaten raw must be superior to that of fish which is to be cooked. Sushi is commonly eaten with condiments like soy sauce and wasabi, a piquant paste made from the grated stem of the Wasabia japonica plant. The dark green seaweed wrappers used in makimono are called nori, which is a type of red algae.\\n\\nSource: https://en.wikipedia.org/wiki/Sushi'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_nl(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Tier 3 Query into Tier 2 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_query(user_query, history=[]):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''Tier 1: Questions related to restaurants, finding food, or food trends. \n",
    "        Examples: \n",
    "        Which restaurants in San Francisco offer dishes with Impossible Meat? (Label - Tier 1) \n",
    "        Give me a summary of the latest trends around desserts. (Tier 1) \n",
    "        Which restaurants are known for sushi? (Tier 1) \n",
    "        Compare the average menu price of vegan restaurants in LA vs. Mexican restaurants. (Tier 1) \n",
    "\n",
    "        Tier 2: Questions about a dish or ingredients. \n",
    "        Examples: \n",
    "        Tell me about biryani. (Tier 2) \t\n",
    "        What is the history of sushi? (Tier 2) \n",
    "        Tell me the contents of sushi. (Tier 2)\n",
    "\n",
    "        The query is a combination of both these classes.\n",
    "\n",
    "        Break down the query I am passing to you below into Tier 1 and Tier 2 questions (with proper context included in both) in the same order, and separate them with commas.\n",
    "\n",
    "        Example: \n",
    "        What is the history of sushi, and which restaurants are known for it?  \n",
    "        Which restaurants are known for sushi?, What is the history of sushi?\n",
    "\n",
    "        Dont include class names in split.\n",
    "\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "     # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_queries = split_query('What is biryani and where can i find it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is biryani?\n",
      "Tier 2\n",
      " Where can I find biryani?\n",
      "Tier 1\n"
     ]
    }
   ],
   "source": [
    "for query in split_queries:\n",
    "    print(query)\n",
    "    print(classify_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If query not relevant to food or ingredients or restaurant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_query(user_query, history=[]):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''My RAG application or chatbot is for answering questions related to restaurants, food, dishes. So basically a one stop solution to find your favourite food or \n",
    "        to know more about it. \n",
    "\n",
    "        So you are the gatekeeper checking if the query given by the user is in context of what we are trying to solve or anything else.\n",
    "\n",
    "        If query is in context just return True if out of context return a funny response stating we might be able to answer that in future. Make sure you refer the history for previous \n",
    "        queries and responses to decide on the context. If you observe the user asking the same out of context question let him know about it in a funny way.\n",
    "\n",
    "        Note -  Only return True that is one word for in context queries  \n",
    "        Make sure you return Only funny message as a response if out of context no need to explain anything else\n",
    "\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "     # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if check_query('Best Sushi spot in town') == str(True): print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_one\n",
      "\n",
      "response_two\n"
     ]
    }
   ],
   "source": [
    "response = \"response_one\" + '\\n\\n' + \"response_two\"\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
