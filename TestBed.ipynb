{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = os.getenv('db_user')\n",
    "db_password = os.getenv('db_password')\n",
    "db_host = os.getenv('db_host')\n",
    "db_name = os.getenv('db_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to download external data from wiki and store in a vector db and retrieve relevant info "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include check within method to check for existing content in vectorDb if so dirctly jump to querying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_embedding(text):\n",
    "\n",
    "    client = openai.OpenAI(api_key = os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding  # Correct way to access embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import streamlit as st\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "\n",
    "def extractData_loadData_performSemanticSearch(info, user_query, history=[]):\n",
    "\n",
    "    query = f\"Select count(*) from metadata where dish_name ILIKE '{info}'\"\n",
    "    insert_query = f\"Insert into metadata Values('{info}')\"\n",
    "    db = SQLDatabase.from_uri(f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\")\n",
    "    execute_query = QuerySQLDataBaseTool(db=db)\n",
    "    count = execute_query.invoke(query)\n",
    "\n",
    "    if count == '[(0,)]':\n",
    "        # Initialize Wikipedia API\n",
    "        wiki_wiki = wikipediaapi.Wikipedia(user_agent='Menudata.ai', language='en')\n",
    "\n",
    "        # Fetch the page\n",
    "        page_py = wiki_wiki.page(info)\n",
    "\n",
    "        # Check if the page exists\n",
    "        if not page_py.exists():\n",
    "            return False\n",
    "\n",
    "        # Function to create chunks from sections and summary\n",
    "        def create_chunks_from_page(page, dish_name):\n",
    "            chunks = []\n",
    "\n",
    "            # Add the page summary as a chunk\n",
    "            summary_chunk = {\n",
    "                \"text\": f\"{page.summary[:200]}\\n\\nSource: {page.canonicalurl}\",  # Append URL\n",
    "                \"metadata\": {\n",
    "                    \"dish_name\": dish_name,\n",
    "                    \"section\": \"Summary\",\n",
    "                }\n",
    "            }\n",
    "            chunks.append(summary_chunk)\n",
    "\n",
    "            # Recursively add sections and subsections as chunks\n",
    "            def add_sections_to_chunks(sections, parent_section=None, level=0):\n",
    "                for s in sections:\n",
    "                    # Create a chunk for the section\n",
    "                    section_title = f\"{parent_section} - {s.title}\" if parent_section else s.title\n",
    "                    section_chunk = {\n",
    "                        \"text\": f\"{s.text[:200]} Source: {page.canonicalurl}\",  # Append URL\n",
    "                        \"metadata\": {\n",
    "                            \"dish_name\": dish_name,\n",
    "                            \"section\": section_title,\n",
    "                        }\n",
    "                    }\n",
    "                    chunks.append(section_chunk)\n",
    "\n",
    "                    # Add subsections recursively, passing the current section as the parent\n",
    "                    add_sections_to_chunks(s.sections, parent_section=s.title, level=level + 1)\n",
    "\n",
    "            # Start processing sections\n",
    "            add_sections_to_chunks(page.sections)\n",
    "\n",
    "            return chunks\n",
    "\n",
    "        # Create chunks from the page\n",
    "        chunks = create_chunks_from_page(page_py, info)\n",
    "        print(\"Chunks created!\")\n",
    "\n",
    "        # Generate embeddings for metadata only\n",
    "        chunk_embeddings = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Convert metadata to a string (e.g., JSON or concatenated string)\n",
    "            metadata_str = f\"{chunk['metadata']['dish_name']} {chunk['metadata']['section']}\"\n",
    "            \n",
    "            # Generate embedding for the metadata string\n",
    "            metadata_embedding = get_embedding(metadata_str)\n",
    "            \n",
    "            chunk_embeddings.append({\n",
    "                \"id\": f\"chunk_{info}{i+1}\",  # Unique ID for each chunk\n",
    "                \"vector\": metadata_embedding,  # Embedding of the metadata\n",
    "                \"metadata\": chunk[\"metadata\"],  # Original metadata\n",
    "                \"text\": chunk[\"text\"]  # Optional: Store text if needed for retrieval\n",
    "            })\n",
    "        print(\"Embeddings created!\")\n",
    "    \n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.Client()\n",
    "\n",
    "    # # Directory where the Chroma DB will be persisted\n",
    "    # persist_directory = \"./chroma_db\"  # Replace with your desired directory\n",
    "\n",
    "    # # Initialize the Chroma client with persistence \n",
    "    # chroma_client = chromadb.Client(\n",
    "    #     Settings(\n",
    "    #     persist_directory=persist_directory,  # Enable persistence\n",
    "    #     chroma_db_impl=\"duckdb+parquet\",      # Use DuckDB with Parquet for storage\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # Create a collection (similar to a table in a traditional DB)\n",
    "    collection_name = 'ExternalInformationTest2'\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "    except:\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "    if count == '[(0,)]':\n",
    "        # Add embeddings to the collection\n",
    "        for chunk in chunk_embeddings:\n",
    "            collection.add(\n",
    "                ids=[chunk[\"id\"]],  # Unique ID for each chunk\n",
    "                embeddings=[chunk[\"vector\"]],  # Embedding of the metadata\n",
    "                metadatas=[chunk[\"metadata\"]],  # Original metadata\n",
    "                documents=[chunk[\"text\"]]  # Optional: Store text if needed for retrieval\n",
    "            )\n",
    "\n",
    "        print(\"Metadata embeddings stored in Chroma!\")\n",
    "        execute_query.invoke(insert_query)\n",
    "\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(' '.join(history)+' '+user_query)\n",
    "\n",
    "    # Perform similarity search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],  # Embedding of the user query\n",
    "        n_results=5 # Number of results to return\n",
    "    )\n",
    "\n",
    "    relevant_documents = results['documents']\n",
    "\n",
    "    return relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created!\n",
      "Embeddings created!\n",
      "Metadata embeddings stored in Chroma!\n"
     ]
    }
   ],
   "source": [
    "result = extractData_loadData_performSemanticSearch('sushi', 'where did sushi originate?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'The earliest written mention of sushi in English described in the Oxford English Dictionary is in an 1893 book, A Japanese Interior, where it mentions sushi as \"a roll of cold rice with fish, sea-weed Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'Until the early 19th century, sushi slowly changed with Japanese cuisine. The Japanese started eating three meals a day, rice was boiled instead of steamed, and of large importance was the development Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'During the Edo period (1603–1867), a third type of sushi, haya-zushi (早寿司、早ずし, \"fast sushi\"), was developed. Haya-zushi differed from earlier sushi in that instead of lactic fermentation of rice, vine Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  ' Source: https://en.wikipedia.org/wiki/Sushi']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to classify user query as Tier 1, 2 or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "\n",
    "def classify_query(user_query, history=[]):\n",
    "    # Step 1: Define the prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''I have a scenario where I want to classify query into a category.\n",
    "\n",
    "        Tier 1: Questions related to restaurants, finding food, or food trends.\n",
    "        Examples:\n",
    "        Which restaurants in San Francisco offer dishes with Impossible Meat? (Label - Tier 1)\n",
    "        Give me a summary of the latest trends around desserts. (Tier 1)\n",
    "        Which restaurants are known for sushi? (Tier 1)\n",
    "        Compare the average menu price of vegan restaurants in LA vs. Mexican restaurants. (Tier 1)\n",
    "        Which food can I find with peas?\n",
    "\n",
    "        Tier 2: Questions about a dish or ingredients.\n",
    "        Examples:\n",
    "        Tell me about biryani. (Tier 2)\n",
    "        What is the history of sushi? (Tier 2)\n",
    "        Tell me the contents of sushi. (Tier 2)\n",
    "        \n",
    "        Tier 3: Questions that combine both restaurant-related and dish-related queries.\n",
    "        Example:\n",
    "        What is the history of sushi, and which restaurants in my area are known for it?\n",
    "\n",
    "        Tell me which class the query falls into with just one word that is the class. Note – might include chat history which should not be considered for classification but only for context\n",
    "\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "    # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/9w2s326d5cx3n3gsd9wtv04h0000gn/T/ipykernel_59654/1204639571.py:37: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
      "/var/folders/7_/9w2s326d5cx3n3gsd9wtv04h0000gn/T/ipykernel_59654/1204639571.py:40: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "/var/folders/7_/9w2s326d5cx3n3gsd9wtv04h0000gn/T/ipykernel_59654/1204639571.py:43: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain.run({\"history\": history, \"user_query\": user_query})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tier 3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_query(\"What is italian cusine and where can i find it near me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 1 - Generate sql query --> Query DB --> Return result in naturla language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Which restaurants in San Francisco offer dishes with Impossible Meat?\",\n",
    "        \"query\": \"SELECT restaurant_name, city FROM restaurants WHERE city = 'San Francisco' AND id IN (SELECT restaurant_id FROM menu_items WHERE menu_description ILIKE '%impossible%');\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find restaurants near me that serve gluten-free pizza.\",\n",
    "        \"query\": \"SELECT restaurant_name FROM restaurants WHERE id IN (SELECT restaurant_id FROM menu_items WHERE menu_description ILIKE '%gluten free%' AND menu_item ILIKE '%pizza%');\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Give me a summary of the latest trends around desserts.\",\n",
    "        \"query\": \"SELECT mi.menu_item AS dessert_name, COUNT(r.id) AS restaurant_count, AVG(r.rating) AS avg_rating, SUM(r.review_count) AS total_reviews FROM menu_items mi JOIN restaurants r ON mi.restaurant_id = r.id WHERE mi.menu_category ILIKE '%dessert%' OR mi.categories ILIKE '%dessert%' GROUP BY mi.menu_item ORDER BY total_reviews DESC, avg_rating DESC LIMIT 10;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Which restaurants are known for Sushi?\",\n",
    "        \"query\": \"SELECT r.restaurant_name, r.rating FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE m.menu_item ILIKE '%sushi%' AND r.rating >= 4.0 GROUP BY r.restaurant_name, r.rating ORDER BY r.rating DESC;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Compare the average menu price of vegan restaurants in LA vs. Mexican restaurants.\",\n",
    "        \"query\": \"SELECT category, AVG(CASE WHEN price = '$' THEN 1 WHEN price = '$$' THEN 2 WHEN price = '$$$' THEN 3 WHEN price = '$$$$' THEN 4 END) AS avg_price_level FROM (SELECT r.id AS restaurant_id, CASE WHEN m.categories ILIKE '%vegan%' THEN 'Vegan' WHEN m.categories ILIKE '%mexican%' THEN 'Mexican' END AS category, r.price FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE r.city = 'Los Angeles' AND (m.categories ILIKE '%vegan%' OR m.categories ILIKE '%mexican%')) subquery GROUP BY category;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Top 5 famous desserts in Boston based on reviews.\",\n",
    "        \"query\": \"SELECT m.menu_item, SUM(r.review_count) AS total_reviews FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE r.city = 'Boston' AND (m.menu_category ILIKE '%dessert%' OR m.categories ILIKE '%dessert%') GROUP BY m.menu_item ORDER BY total_reviews DESC LIMIT 5;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Average rating of Mexican and Italian restaurants.\",\n",
    "        \"query\": \"SELECT CASE WHEN m.categories ILIKE '%mexican%' THEN 'Mexican' WHEN m.categories ILIKE '%italian%' THEN 'Italian' END AS cuisine, AVG(r.rating) AS avg_rating FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id AND (m.categories ILIKE '%mexican%' OR m.categories ILIKE '%italian%') GROUP BY cuisine;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Restaurants with the most vegetarian options.\",\n",
    "        \"query\": \"SELECT r.restaurant_name, r.city, COUNT(m.item_id) AS vegetarian_item_count FROM restaurants r JOIN menu_items m ON r.id = m.restaurant_id WHERE m.categories ILIKE '%vegetarian%' GROUP BY r.restaurant_name, r.city ORDER BY vegetarian_item_count DESC LIMIT 5;\"\n",
    "    }\n",
    "]\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def get_example_selector():\n",
    "    example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "        examples,\n",
    "        OpenAIEmbeddings(),\n",
    "        Chroma,\n",
    "        k=3,\n",
    "        input_keys=[\"input\"],\n",
    "    )\n",
    "    return example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate,PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "def tierOne(user_query, history=[]):\n",
    "    \n",
    "    select_table = ['menu_items', 'restaurants']\n",
    "    db = SQLDatabase.from_uri(f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\", include_tables = select_table)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\\nSQLQuery:\"),\n",
    "        (\"ai\", \"{query}\"),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=get_example_selector(),\n",
    "    input_variables=[\"input\", 'top_k'],\n",
    "    )\n",
    "\n",
    "    final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a PostgreSQL expert. Given an input question, create a syntactically correct PostgreSQL query to run. Unless otherwise specificed.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\"),\n",
    "        few_shot_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "    Chat History:{history}\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer: \"\"\"\n",
    "    )\n",
    "\n",
    "    generate_query = create_sql_query_chain(llm, db,final_prompt) \n",
    "    execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "    rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # chain = (\n",
    "    # RunnablePassthrough.assign(query=generate_query).assign(\n",
    "    #     result=itemgetter(\"query\") | execute_query\n",
    "    # )\n",
    "    # | rephrase_answer )\n",
    "\n",
    "    chain = (\n",
    "    RunnablePassthrough.assign(query=generate_query)\n",
    "    .assign(clean_query=lambda x: clean_sql_query(x['query']))\n",
    "    .assign(result=itemgetter(\"clean_query\") | execute_query\n",
    "    )\n",
    "    | rephrase_answer)\n",
    "\n",
    "    return chain.invoke({'question':user_query, 'history':history})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 09:54:55.449 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-06 09:54:55.480 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-06 09:54:55.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-06 09:54:55.481 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-06 09:54:55.986 Thread 'Thread-27': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-06 09:54:55.987 Thread 'Thread-27': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-06 09:54:56.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-06 09:54:56.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "answer = tierOne('Compare the average menu price of vegan restaurants vs. Mexican restaurants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average menu price level for vegan restaurants is 2.00, while for Mexican restaurants, it is approximately 1.83. This indicates that, on average, vegan restaurants have a slightly higher menu price level compared to Mexican restaurants.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract ingredinet or dish from user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "\n",
    "def extract_info(user_query, history=[]):\n",
    "    # Step 1: Define the prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''You know that the query below is asking information about some dish or ingredient. Identify the ingredient or dish name and return only that value.\n",
    "        Examples -\n",
    "        Tell me about biryani. Biryani \n",
    "        What is the history of sushi? Sushi\n",
    "        Tell me about Saffron. Saffron\n",
    "        Below is the query for you to identify and includes chat history if any,\n",
    "        Note - Add a underscore if multiword \n",
    "        \n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "    # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pasta'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('What is the most common ingredients used in pasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use extractData_loadData_performSemanticSearch method to retrieve relevant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the most common ingredients used in sushi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = extractData_loadData_performSemanticSearch(extract_info(query), query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['All sushi has a base of specially prepared rice, complemented with other ingredients. Traditional Japanese sushi consists of rice flavored with vinegar sauce and various raw or cooked ingredients. Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'Sushi-meshi (鮨飯) (also known as su-meshi (酢飯), shari (舎利), or gohan (ご飯)) is a preparation of white, short-grained, Japanese rice mixed with a dressing consisting of rice vinegar, sugar, salt, and occ Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'Sushi is commonly eaten with condiments. Sushi may be dipped in shōyu (soy sauce), and is usually flavored with wasabi, a piquant paste made from the grated stem of the Wasabia japonica plant. Japanes Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'The dark green seaweed wrappers used in makimono are called nori (海苔). Nori is a type of red algae, typically in the family Bangiaceae, traditionally cultivated in the harbors of Japan. Originally, al Source: https://en.wikipedia.org/wiki/Sushi',\n",
       "  'The ingredients used inside sushi are called gu and are, typically, varieties of fish. For culinary, sanitary, and aesthetic reasons, the minimum quality and freshness of fish to be eaten raw must be  Source: https://en.wikipedia.org/wiki/Sushi']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer query using context found above and return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_nl(context, user_query, history = []):\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"history\", \"user_query\"],\n",
    "        template='''Using relevant context passed answer the user query. If history of chat availableuse it for relevant context. Note - After answering the query provide relevant source\n",
    "        Relevant Context - {context}\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "     # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"context\": context, \"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common ingredients used in sushi are specially prepared rice known as Sushi-meshi, which is a preparation of white, short-grained, Japanese rice mixed with a dressing consisting of rice vinegar, sugar, salt. The ingredients used inside sushi are called gu and are typically varieties of fish. The minimum quality and freshness of fish to be eaten raw must be superior to that of fish which is to be cooked. Sushi is commonly eaten with condiments like soy sauce and wasabi, a piquant paste made from the grated stem of the Wasabia japonica plant. The dark green seaweed wrappers used in makimono are called nori, which is a type of red algae.\\n\\nSource: https://en.wikipedia.org/wiki/Sushi'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_nl(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tier 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Tier 3 Query into Tier 2 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_query(user_query, history=[]):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''Tier 1: Questions related to restaurants, finding food, or food trends. \n",
    "        Examples: \n",
    "        Which restaurants in San Francisco offer dishes with Impossible Meat? (Label - Tier 1) \n",
    "        Give me a summary of the latest trends around desserts. (Tier 1) \n",
    "        Which restaurants are known for sushi? (Tier 1) \n",
    "        Compare the average menu price of vegan restaurants in LA vs. Mexican restaurants. (Tier 1) \n",
    "\n",
    "        Tier 2: Questions about a dish or ingredients. \n",
    "        Examples: \n",
    "        Tell me about biryani. (Tier 2) \t\n",
    "        What is the history of sushi? (Tier 2) \n",
    "        Tell me the contents of sushi. (Tier 2)\n",
    "\n",
    "        The query is a combination of both these classes.\n",
    "\n",
    "        Break down the query I am passing to you below into Tier 1 and Tier 2 questions (with proper context included in both) in the same order, and separate them with commas.\n",
    "\n",
    "        Example: \n",
    "        What is the history of sushi, and which restaurants are known for it?  \n",
    "        Which restaurants are known for sushi?, What is the history of sushi?\n",
    "\n",
    "        Dont include class names in split.\n",
    "\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "     # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_queries = split_query('What is biryani and where can i find it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is biryani?\n",
      "Tier 2\n",
      " Where can I find biryani?\n",
      "Tier 1\n"
     ]
    }
   ],
   "source": [
    "for query in split_queries:\n",
    "    print(query)\n",
    "    print(classify_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If query not relevant to food or ingredients or restaurant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_query(user_query, history=[]):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"user_query\"],\n",
    "        template='''My RAG application or chatbot is for answering questions related to restaurants, food, dishes. So basically a one stop solution to find your favourite food or \n",
    "        to know more about it. \n",
    "\n",
    "        So you are the gatekeeper checking if the query given by the user is in context of what we are trying to solve or anything else.\n",
    "\n",
    "        If query is in context just return True if out of context return a funny response stating we might be able to answer that in future. Make sure you refer the history for previous \n",
    "        queries and responses to decide on the context. If you observe the user asking the same out of context question let him know about it in a funny way.\n",
    "\n",
    "        Note -  Only return True that is one word for in context queries  \n",
    "        Make sure you return Only funny message as a response if out of context no need to explain anything else\n",
    "\n",
    "        history - {history}\n",
    "        query - {user_query}'''\n",
    "    )\n",
    "\n",
    "     # Step 2: Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)  # Use \"gpt-4\" instead of \"gpt-4o\"\n",
    "\n",
    "    # Step 3: Create the LLMChain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Step 4: Pass the user query and history to the chain\n",
    "    response = chain.run({\"history\": history, \"user_query\": user_query})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if check_query('Best Sushi spot in town') == str(True): print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_one\n",
      "\n",
      "response_two\n"
     ]
    }
   ],
   "source": [
    "response = \"response_one\" + '\\n\\n' + \"response_two\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: guardrails-ai in /opt/anaconda3/lib/python3.11/site-packages (0.6.2)\n",
      "Requirement already satisfied: boto3<2,>1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.34.162)\n",
      "Requirement already satisfied: coloredlogs<16.0.0,>=15.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (15.0.1)\n",
      "Requirement already satisfied: diff-match-patch<20230431,>=20230430 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (20230430)\n",
      "Requirement already satisfied: faker<26.0.0,>=25.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (25.9.2)\n",
      "Requirement already satisfied: griffe<0.37.0,>=0.36.9 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.36.9)\n",
      "Requirement already satisfied: guardrails-api-client<0.5.0,>=0.4.0a1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.4.0a1)\n",
      "Requirement already satisfied: guardrails-hub-types<0.0.5,>=0.0.4 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.0.4)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (4.23.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.2.38)\n",
      "Requirement already satisfied: litellm<2.0.0,>=1.37.14 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.49.1)\n",
      "Requirement already satisfied: lxml<5.0.0,>=4.9.3 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (4.9.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.30.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.56.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: pip>=22 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (24.3.1)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.10.3)\n",
      "Requirement already satisfied: pydash<8.0.0,>=7.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (7.0.7)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.9.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (13.9.3)\n",
      "Requirement already satisfied: rstr<4.0.0,>=3.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (3.2.2)\n",
      "Requirement already satisfied: semver<4.0.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (3.0.2)\n",
      "Requirement already satisfied: tenacity>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.7.0)\n",
      "Requirement already satisfied: typer<0.13,>=0.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer[all]<0.13,>=0.9.0->guardrails-ai) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (4.12.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.162 in /opt/anaconda3/lib/python3.11/site-packages (from boto3<2,>1->guardrails-ai) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from boto3<2,>1->guardrails-ai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from boto3<2,>1->guardrails-ai) (0.10.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.11/site-packages (from coloredlogs<16.0.0,>=15.0.1->guardrails-ai) (10.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/lib/python3.11/site-packages (from griffe<0.37.0,>=0.36.9->guardrails-ai) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai) (75.3.0)\n",
      "Requirement already satisfied: urllib3<2.1.0,>=1.25.3 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai) (2.0.7)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (0.20.1)\n",
      "Requirement already satisfied: fqdn in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.5.1)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (3.10)\n",
      "Requirement already satisfied: isoduration in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (3.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (0.1.4)\n",
      "Requirement already satisfied: rfc3987 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.3.8)\n",
      "Requirement already satisfied: uri-template in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (24.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (0.1.86)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (24.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.11.0b0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.4.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.1.4)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (1.0.1)\n",
      "Requirement already satisfied: tokenizers in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (0.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (0.4.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.66.6)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.64.3)\n",
      "Requirement already satisfied: opentelemetry-api~=1.15 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.19 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (4.25.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->guardrails-ai) (0.48b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (2.18.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken>=0.5.1->guardrails-ai) (2024.9.11)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<0.13,>=0.9.0->typer[all]<0.13,>=0.9.0->guardrails-ai) (1.5.4)\n",
      "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wrapt<2,>=1.10 in /opt/anaconda3/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.17.0rc1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.30.1->guardrails-ai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.30.1->guardrails-ai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.0.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.4,>=0.1->guardrails-ai) (3.10.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai) (0.1.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.17.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.11/site-packages (from tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (2024.10.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install guardrails-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: guardrails-ai in /opt/anaconda3/lib/python3.11/site-packages (0.6.2)\n",
      "Requirement already satisfied: boto3<2,>1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.34.162)\n",
      "Requirement already satisfied: coloredlogs<16.0.0,>=15.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (15.0.1)\n",
      "Requirement already satisfied: diff-match-patch<20230431,>=20230430 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (20230430)\n",
      "Requirement already satisfied: faker<26.0.0,>=25.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (25.9.2)\n",
      "Requirement already satisfied: griffe<0.37.0,>=0.36.9 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.36.9)\n",
      "Requirement already satisfied: guardrails-api-client<0.5.0,>=0.4.0a1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.4.0a1)\n",
      "Requirement already satisfied: guardrails-hub-types<0.0.5,>=0.0.4 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.0.4)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (4.23.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.2.38)\n",
      "Requirement already satisfied: litellm<2.0.0,>=1.37.14 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.49.1)\n",
      "Requirement already satisfied: lxml<5.0.0,>=4.9.3 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (4.9.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.30.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.56.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: pip>=22 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (24.3.1)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.10.3)\n",
      "Requirement already satisfied: pydash<8.0.0,>=7.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (7.0.7)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.9.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (13.9.3)\n",
      "Requirement already satisfied: rstr<4.0.0,>=3.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (3.2.2)\n",
      "Requirement already satisfied: semver<4.0.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (3.0.2)\n",
      "Requirement already satisfied: tenacity>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (0.7.0)\n",
      "Requirement already satisfied: typer<0.13,>=0.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer[all]<0.13,>=0.9.0->guardrails-ai) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-ai) (4.12.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.162 in /opt/anaconda3/lib/python3.11/site-packages (from boto3<2,>1->guardrails-ai) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from boto3<2,>1->guardrails-ai) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from boto3<2,>1->guardrails-ai) (0.10.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.11/site-packages (from coloredlogs<16.0.0,>=15.0.1->guardrails-ai) (10.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/lib/python3.11/site-packages (from griffe<0.37.0,>=0.36.9->guardrails-ai) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai) (75.3.0)\n",
      "Requirement already satisfied: urllib3<2.1.0,>=1.25.3 in /opt/anaconda3/lib/python3.11/site-packages (from guardrails-api-client<0.5.0,>=0.4.0a1->guardrails-ai) (2.0.7)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (0.20.1)\n",
      "Requirement already satisfied: fqdn in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.5.1)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (3.10)\n",
      "Requirement already satisfied: isoduration in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (3.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (0.1.4)\n",
      "Requirement already satisfied: rfc3987 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.3.8)\n",
      "Requirement already satisfied: uri-template in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (24.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (0.1.86)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->guardrails-ai) (24.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.11.0b0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (8.4.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (3.1.4)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (1.0.1)\n",
      "Requirement already satisfied: tokenizers in /opt/anaconda3/lib/python3.11/site-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai) (0.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (0.4.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.30.1->guardrails-ai) (4.66.6)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.64.3)\n",
      "Requirement already satisfied: opentelemetry-api~=1.15 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.27.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.19 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (4.25.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /opt/anaconda3/lib/python3.11/site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->guardrails-ai) (0.48b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0,>=2.0.0->guardrails-ai) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (2.18.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken>=0.5.1->guardrails-ai) (2024.9.11)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<0.13,>=0.9.0->typer[all]<0.13,>=0.9.0->guardrails-ai) (1.5.4)\n",
      "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wrapt<2,>=1.10 in /opt/anaconda3/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai) (1.17.0rc1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.30.1->guardrails-ai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.30.1->guardrails-ai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.0.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.4,>=0.1->guardrails-ai) (3.10.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai) (0.1.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm<2.0.0,>=1.37.14->guardrails-ai) (1.17.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai) (1.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.11/site-packages (from tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.37.14->guardrails-ai) (2024.10.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install guardrails-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard, OnFailAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation failed for field with errors: The following sentences in your response were found to be toxic:\n",
      "\n",
      "- Shut the hell up!\n"
     ]
    }
   ],
   "source": [
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import ToxicLanguage\n",
    "\n",
    "guard = Guard().use_many(\n",
    "    ToxicLanguage(threshold=0.5, validation_method=\"sentence\", on_fail=OnFailAction.EXCEPTION)\n",
    ")\n",
    "\n",
    "try:\n",
    "    guard.validate(\n",
    "        \"\"\"Shut the hell up! Apple just released a new iPhone.\"\"\"\n",
    "    )  # Both the guardrails fail\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Stripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sql_query(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SQL query by removing code block syntax, various SQL tags, backticks,\n",
    "    prefixes, and unnecessary whitespace while preserving the core SQL query.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw SQL query text that may contain code blocks, tags, and backticks\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned SQL query\n",
    "    \"\"\"\n",
    "    # Step 1: Remove code block syntax and any SQL-related tags\n",
    "    # This handles variations like ```sql, ```SQL, ```SQLQuery, etc.\n",
    "    block_pattern = r\"```(?:sql|SQL|SQLQuery|mysql|postgresql)?\\s*(.*?)\\s*```\"\n",
    "    text = re.sub(block_pattern, r\"\\1\", text, flags=re.DOTALL)\n",
    "\n",
    "    # Step 2: Handle \"SQLQuery:\" prefix and similar variations\n",
    "    # This will match patterns like \"SQLQuery:\", \"SQL Query:\", \"MySQL:\", etc.\n",
    "    prefix_pattern = r\"^(?:SQL\\s*Query|SQLQuery|MySQL|PostgreSQL|SQL)\\s*:\\s*\"\n",
    "    text = re.sub(prefix_pattern, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 3: Extract the first SQL statement if there's random text after it\n",
    "    # Look for a complete SQL statement ending with semicolon\n",
    "    sql_statement_pattern = r\"(SELECT.*?;)\"\n",
    "    sql_match = re.search(sql_statement_pattern, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if sql_match:\n",
    "        text = sql_match.group(1)\n",
    "\n",
    "    # Step 4: Remove backticks around identifiers\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "\n",
    "    # Step 5: Normalize whitespace\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Step 6: Preserve newlines for main SQL keywords to maintain readability\n",
    "    keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'HAVING', 'ORDER BY',\n",
    "               'LIMIT', 'JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'INNER JOIN',\n",
    "               'OUTER JOIN', 'UNION', 'VALUES', 'INSERT', 'UPDATE', 'DELETE']\n",
    "\n",
    "    # Case-insensitive replacement for keywords\n",
    "    pattern = '|'.join(r'\\b{}\\b'.format(k) for k in keywords)\n",
    "    text = re.sub(f'({pattern})', r'\\n\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 7: Final cleanup\n",
    "    # Remove leading/trailing whitespace and extra newlines\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
